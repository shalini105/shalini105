{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8222f44e",
   "metadata": {},
   "source": [
    "### Q1. Difference between Simple Linear Regression and Multiple Linear Regression\n",
    "\n",
    "- **Simple Linear Regression:** Involves one independent variable to predict a dependent variable.\n",
    "    - **Example:** Predicting a student's final exam score based on the number of study hours.\n",
    "  \n",
    "- **Multiple Linear Regression:** Involves two or more independent variables to predict a dependent variable.\n",
    "    - **Example:** Predicting a house price based on square footage, number of bedrooms, and location.\n",
    "\n",
    "### Q2. Assumptions of Linear Regression\n",
    "\n",
    "1. **Linearity:** The relationship between independent and dependent variables must be linear.\n",
    "2. **Independence:** Observations must be independent of each other.\n",
    "3. **Homoscedasticity:** Constant variance of errors across all levels of the independent variable.\n",
    "4. **Normality of residuals:** Residuals (errors) should be normally distributed.\n",
    "5. **No multicollinearity:** In multiple regression, independent variables should not be highly correlated.\n",
    "\n",
    "To check assumptions:\n",
    "- **Linearity:** Use scatter plots.\n",
    "- **Homoscedasticity:** Plot residuals vs. fitted values.\n",
    "- **Normality:** Plot residuals on a histogram or use a Q-Q plot.\n",
    "- **Multicollinearity:** Check Variance Inflation Factor (VIF).\n",
    "\n",
    "### Q3. Interpretation of Slope and Intercept in Linear Regression\n",
    "\n",
    "- **Slope:** Represents the change in the dependent variable for a one-unit change in the independent variable.\n",
    "    - **Example:** If a regression model predicts salary based on years of experience, a slope of 2000 means that for each additional year of experience, salary increases by $2000.\n",
    "  \n",
    "- **Intercept:** Represents the predicted value of the dependent variable when the independent variable is zero.\n",
    "    - **Example:** In the same model, an intercept of 30000 would indicate that with zero years of experience, the predicted starting salary is $30,000.\n",
    "\n",
    "### Q4. Gradient Descent in Machine Learning\n",
    "\n",
    "Gradient Descent is an optimization algorithm used to minimize the cost function in machine learning models. It works by iteratively adjusting the model’s parameters in the direction of the negative gradient to find the minimum error.\n",
    "\n",
    "In **linear regression**, gradient descent is used to minimize the difference between predicted and actual values by adjusting the model parameters (slope and intercept).\n",
    "\n",
    "### Q5. Multiple Linear Regression Model\n",
    "\n",
    "Multiple linear regression predicts a dependent variable using two or more independent variables. It models the relationship between one dependent and several independent variables.\n",
    "\n",
    "**Difference from Simple Linear Regression:** While simple linear regression deals with one independent variable, multiple linear regression includes several independent variables, giving it more complexity and better explanatory power for certain problems.\n",
    "\n",
    "### Q6. Multicollinearity in Multiple Linear Regression\n",
    "\n",
    "- **Multicollinearity** occurs when two or more independent variables are highly correlated, making it difficult to determine the individual impact of each variable.\n",
    "  \n",
    "- **Detection:** Can be detected using metrics such as the Variance Inflation Factor (VIF). A VIF value above 10 suggests multicollinearity.\n",
    "  \n",
    "- **Addressing Multicollinearity:** You can address it by:\n",
    "    - Removing one of the correlated variables.\n",
    "    - Using regularization techniques like Ridge regression.\n",
    "    - Combining correlated variables into a single feature.\n",
    "\n",
    "### Q7. Polynomial Regression Model\n",
    "\n",
    "**Polynomial Regression** models the relationship between the dependent and independent variables as an nth degree polynomial, allowing it to fit more complex curves than a linear regression.\n",
    "\n",
    "**Difference from Linear Regression:** While linear regression assumes a straight-line relationship, polynomial regression can model nonlinear relationships, providing flexibility for curving trends in the data.\n",
    "\n",
    "### Q8. Advantages and Disadvantages of Polynomial Regression\n",
    "\n",
    "- **Advantages:**\n",
    "    - Can fit complex, nonlinear relationships.\n",
    "    - Provides more flexibility for data that doesn’t follow a straight line.\n",
    "\n",
    "- **Disadvantages:**\n",
    "    - Prone to overfitting, especially if the degree of the polynomial is too high.\n",
    "    - Can be less interpretable due to the complexity of the model.\n",
    "\n",
    "- **When to Use:** Polynomial regression is preferred when the relationship between variables is nonlinear, such as modeling population growth or the trajectory of objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738ec86a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
