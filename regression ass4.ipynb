{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ff95e7a",
   "metadata": {},
   "source": [
    "### Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "**Lasso Regression** (Least Absolute Shrinkage and Selection Operator) is a type of linear regression that applies L1 regularization to reduce model complexity by penalizing the absolute size of regression coefficients. The main difference between Lasso and other regression techniques (like Ridge Regression) is that Lasso can shrink some coefficients to exactly zero, effectively performing **feature selection**.\n",
    "\n",
    "### Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
    "The primary advantage of Lasso Regression is its ability to perform automatic **feature selection** by shrinking the coefficients of less important features to zero. This makes the model simpler and helps reduce overfitting, especially when there are many irrelevant or redundant features in the dataset.\n",
    "\n",
    "### Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
    "In a Lasso Regression model, coefficients that are exactly **zero** indicate features that are not contributing to the model and have been excluded (selected out) by the algorithm. Non-zero coefficients represent the remaining features that are influencing the outcome. The magnitude of the non-zero coefficients indicates the strength of the relationship between the feature and the target variable.\n",
    "\n",
    "### Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?\n",
    "The main tuning parameter in Lasso Regression is the **regularization parameter** (λ or alpha). This parameter controls the strength of the penalty on the coefficients:\n",
    "- **Higher λ**: More regularization, leading to more coefficients being shrunk to zero and a simpler model with fewer features.\n",
    "- **Lower λ**: Less regularization, allowing more features to remain in the model.\n",
    "Choosing the right λ is important because too much regularization can underfit the model, while too little regularization can lead to overfitting.\n",
    "\n",
    "### Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "Lasso Regression by itself is designed for linear relationships, but it can be extended to handle non-linear problems by combining it with **polynomial features** or other non-linear transformations of the input data. By transforming the features to capture non-linear relationships and then applying Lasso, it can be used effectively in non-linear contexts.\n",
    "\n",
    "### Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
    "The key difference between **Ridge Regression** and **Lasso Regression** lies in their regularization techniques:\n",
    "- **Ridge Regression** uses **L2 regularization**, which penalizes the sum of the squared coefficients. It shrinks the coefficients but never reduces them to zero, so all features remain in the model.\n",
    "- **Lasso Regression** uses **L1 regularization**, which penalizes the sum of the absolute values of the coefficients. It can shrink coefficients to exactly zero, enabling automatic feature selection.\n",
    "Thus, Lasso is often preferred for feature selection, while Ridge is used when all features are believed to contribute but need regularization.\n",
    "\n",
    "### Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
    "Yes, Lasso Regression can handle **multicollinearity** (when input features are highly correlated). It does this by shrinking the coefficients of the less important correlated features to zero, effectively selecting one of the correlated features and reducing the effect of multicollinearity. However, Ridge Regression is generally considered better at handling multicollinearity because it penalizes large coefficients for all correlated features, rather than eliminating them.\n",
    "\n",
    "### Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\n",
    "The optimal value of λ can be chosen using **cross-validation**. Techniques like **k-fold cross-validation** help determine which value of λ provides the best model performance by evaluating the model on different subsets of the data. This ensures that the model is neither overfitting nor underfitting. The λ value that minimizes the cross-validation error is typically selected as the optimal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7959e3a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
